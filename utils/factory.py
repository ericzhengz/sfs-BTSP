import math


def _arg_get(args, key, default=None):
    if isinstance(args, dict):
        return args.get(key, default)
    return getattr(args, key, default)


def get_model(model_name, args):
    name = model_name.lower()
    if name == "simplecil":
        from models.simplecil import Learner
    elif name == "aper_finetune":
        from models.aper_finetune import Learner
    elif name == "aper_ssf":
        from models.aper_ssf import Learner
    elif name == "aper_vpt":
        from models.aper_vpt import Learner 
    elif name == "aper_adapter":
        from models.aper_adapter import Learner
    elif name == "l2p":
        from models.l2p import Learner
    elif name == "dualprompt":
        from models.dualprompt import Learner
    elif name == "coda_prompt":
        from models.coda_prompt import Learner
    elif name == "finetune":
        from models.finetune import Learner
    elif name == "icarl":
        from models.icarl import Learner
    elif name == "der":
        from models.der import Learner
    elif name == "coil":
        from models.coil import Learner
    elif name == "foster":
        from models.foster import Learner
    elif name == "memo":
        from models.memo import Learner
    elif name == 'ranpac':
        from models.ranpac import Learner
    elif name == "ease":
        from models.ease import Learner
    elif name == 'slca':
        from models.slca import Learner
    elif name == 'lae':
        from models.lae import Learner
    elif name == 'fecam':
        from models.fecam import Learner
    elif name == 'dgr':
        from models.dgr import Learner
    elif name == 'mos':
        from models.mos import Learner
    elif name == 'cofima':
        from models.cofima import Learner
    elif name == 'duct':
        from models.duct import Learner
    elif name == 'tuna':
        from models.tuna import Learner
    else:
        assert 0
    
    # Build baseline model
    inner = Learner(args)
    
    # Wrap with BTSP plugin if enabled
    if _arg_get(args, "use_btsp", False):
        from models.plugins.btsp_plugins import BTSPPlugin
        from models.plugins.unified_metrics import ResourceBudget
        
        # Resolve SSOT early (non-fatal here)
        cfg_norm = None
        try:
            if isinstance(args, dict):
                from btsp.config_resolver import resolve_btsp_config
                cfg_norm, _ = resolve_btsp_config(args, strict=False)
        except Exception:
            cfg_norm = None
        
        # 1) ç§»é™¤å¼ºåˆ¶nb_classesè¦æ±‚ï¼Œè®©BTSPPluginçš„æ™ºèƒ½æ¨æ–­é€»è¾‘å¤„ç†
        num_classes = _arg_get(args, "nb_classes", _arg_get(args, "init_cls", None))
        # æ³¨æ„ï¼šä¸å†å¼ºåˆ¶è¦æ±‚num_classesï¼ŒBTSPPluginä¼šé€šè¿‡data_manageræ¨æ–­

        # 2) ç»Ÿä¸€ tau_e_steps å‚æ•°å‘½åï¼ˆä¼˜å…ˆè§„èŒƒé”®ï¼‰
        tau_e_steps = None
        if cfg_norm is not None and ("btsp.memory.init.tau_e_steps" in cfg_norm):
            tau_e_steps = cfg_norm.get("btsp.memory.init.tau_e_steps")
        if tau_e_steps is None:
            tau_e_steps = _arg_get(args, "btsp_tau_e_steps", _arg_get(args, "btsp_tau_e", None))
        if tau_e_steps is None:
            T_eff_cfg = _arg_get(args, "btsp_T_eff", None)
            if T_eff_cfg is not None:
                theta_cfg = float(_arg_get(args, "btsp_theta", _arg_get(args, "btsp_theta_init", 0.2)))
                denom = max(math.log(1.0 / max(theta_cfg, 1e-8)), 1e-8)
                tau_e_steps = float(T_eff_cfg) / denom
            else:
                tau_e_steps = 4.0

        # 3) é²æ£’è·å–ç‰¹å¾ç»´åº¦ï¼šå°è¯• feature_dim / out_dim / é…ç½® / é»˜è®¤ 768
        net = getattr(inner, "_network", None)
        feat_dim = None
        if net is not None:
            feat_dim = getattr(net, "feature_dim", None)
            if feat_dim is None:
                feat_dim = getattr(net, "out_dim", None)
            
        if feat_dim is None:
            feat_dim = _arg_get(args, "feat_dim", 768)
            
        # 4) æ–°å¢ï¼šäº‹ä»¶é©±åŠ¨æ§åˆ¶ç³»ç»Ÿå‚æ•°ï¼ˆä¸å†ä»è¿™é‡Œä¼ é€’ target_epsilonï¼‰
        control_mode = _arg_get(args, "btsp_control_mode", "A")  # A=Analysis-only, B=Intervention
        enable_unified_protocol = _arg_get(args, "btsp_enable_unified_protocol", _arg_get(args, "btsp_use_unified_eval", True))
        
        # 5) èµ„æºé¢„ç®—é…ç½®
        resource_budget = None
        if enable_unified_protocol:
            resource_budget = ResourceBudget(
                max_flops=_arg_get(args, "budget_max_flops", 1e12),
                max_memory_mb=_arg_get(args, "budget_max_memory_mb", 8192),
                max_bytes=_arg_get(args, "budget_max_bytes", 1e9),
                max_train_time=_arg_get(args, "budget_max_train_time", 3600),
                max_infer_time=_arg_get(args, "budget_max_infer_time", 100)
            )

        return BTSPPlugin(
            inner,
            num_classes=num_classes,
            feat_dim=feat_dim,
            args=args,  # ğŸ”¥ ä¼ é€’å®Œæ•´çš„argsï¼Œè®©BTSPPluginè‡ªå·±è§£æå‚æ•°
            # ä¿ç•™å…³é”®å‚æ•°çš„æ˜¾å¼ä¼ é€’ä»¥ä¿è¯å…¼å®¹æ€§
            N_bits=_arg_get(args, "btsp_N_bits", 8192),
            topk=_arg_get(args, "btsp_topk", 256),
            theta=_arg_get(args, "btsp_theta", 0.2),
            tau_e_steps=tau_e_steps,
            branches=_arg_get(args, "btsp_branches", 8),
            alpha=_arg_get(args, "btsp_alpha", 0.5),
            homeo_interval=_arg_get(args, "btsp_homeo_interval", 100),
            zstats_interval=_arg_get(args, "btsp_zstats_interval", 30),
            # æ–°å¢äº‹ä»¶é©±åŠ¨æ§åˆ¶å‚æ•°
            control_mode=control_mode,
            target_epsilon=None,  # SSOT ç”±æ’ä»¶è§£æ
            enable_unified_protocol=enable_unified_protocol,
            resource_budget=resource_budget,
            # å®éªŒå‚æ•°
            experiment_logging=_arg_get(args, "btsp_enable_unified_logging", False),
        )
    return inner