# btsp/memory.py
from __future__ import annotations
import math, torch, logging
from torch import nn

class BTSPMemory(nn.Module):
    """
    SFS-BTSPï¼ˆQ=2ï¼‰å¯æ’è®°å¿†å±‚ï¼š
      çŠ¶æ€: S[C,N] boolï¼›èµ„æ ¼è¿¹ e[N]ï¼›åˆ†æ”¯æ˜ å°„ branch_of[N]ï¼›åˆ†æ”¯é—¨æ§ p_gate[B]
      æ£€ç´¢: popcount -> z-score -> temperature
      å†™å…¥: æŒ‡æ•°è¡°å‡èµ„æ ¼è¿¹ Ã— åˆ†æ”¯é—¨æ§ Ã— 0.5 éšæœºç¿»è½¬ï¼ˆXORï¼‰ï¼Œä¸”"æŒ‰ç±»æˆªæ–­"
      è®¾å¤‡: è®°å¿†å±‚çŠ¶æ€å¸¸é©» CPUï¼›æ— åä¼ 
    """
    def __init__(self, num_classes: int, num_bits: int, num_branches: int = 1,
                 theta: float = 0.2, tau_e_steps: float = 4.0, temperature: float = 1.5,
                 z_momentum: float = 0.99, zstats_interval: int = 30, device: str = "cpu"):
        super().__init__()
        assert num_classes > 0 and num_bits > 0 and num_branches > 0
        self.num_classes = num_classes
        self.num_bits = num_bits
        self.num_branches = num_branches
        self.device_type = device
        assert device in ["cpu", "cuda"]

        self.temperature = temperature
        self.z_momentum = z_momentum
        self.zstats_interval = zstats_interval

        # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šç§»é™¤é˜ˆå€¼å‚æ•°ï¼Œä»…ä¿ç•™åˆ†ç¦»æ€§é˜ˆå€¼ç”¨äºå…¶ä»–åŠŸèƒ½
        # self.tau_sat = 0.95                  # å·²ç§»é™¤ï¼šçª—å£å‘½ä¸­ç›®æ ‡
        # self.c_occ = 1.5                     # å·²ç§»é™¤ï¼šå ç”¨é¢„ç®—ç³»æ•°
        # self.q_max_cls = 0.10                # å·²ç§»é™¤ï¼šé€ç±»ç¡¬åœé˜ˆå€¼
        # self.q_max_global = 0.50             # å·²ç§»é™¤ï¼šå…¨å±€ä¿é™©é˜ˆå€¼
        self.separability_threshold = 1.5     # ä¿ç•™ï¼šå¯åˆ†æ€§z-scoreä¸‹é™ï¼ˆç”¨äºå…¶ä»–åŠŸèƒ½ï¼‰

        # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šç§»é™¤é¢„ç®—æ§åˆ¶å‚æ•°
        # self.delta_q_max = 0.05              # å·²ç§»é™¤ï¼šé¢„ç®—æ§åˆ¶
        # self.enable_incremental_budget = True # å·²ç§»é™¤ï¼šé¢„ç®—æ§åˆ¶

        # æ³¨å†ŒçŠ¶æ€å¼ é‡...
        self.register_buffer("S", torch.zeros((num_classes, num_bits), dtype=torch.bool, device=device))
        self.register_buffer("e", torch.zeros((num_classes, num_bits), dtype=torch.float32, device=device))
        self.register_buffer("tau_e_steps", torch.tensor(tau_e_steps, dtype=torch.float32, device=device))
        self.register_buffer("theta", torch.tensor(theta, dtype=torch.float32, device=device))

        # åˆ†æ”¯-ç±»æ˜ å°„
        branch_assignment = torch.arange(num_classes, device=device) % num_branches
        self.register_buffer("branch_assignment", branch_assignment)
        
        # åˆ†æ”¯-ä½æ˜ å°„ï¼ˆå…¼å®¹è¯Šæ–­è„šæœ¬ï¼‰
        self.register_buffer("branch_of", torch.randint(0, num_branches, (num_bits,), device=device))

        # é—¨æ§å‚æ•°
        gate_init = min(0.01, 1.0 / (num_classes + 1))
        self.register_buffer("p_gate", torch.full((num_branches,), gate_init, dtype=torch.float32, device=device))

        # Z-score ç»Ÿè®¡ï¼ˆæ£€ç´¢æ ‡å‡†åŒ–ï¼‰
        self.register_buffer("z_mu", torch.zeros(num_classes, dtype=torch.float32, device=device))
        self.register_buffer("z_std", torch.ones(num_classes, dtype=torch.float32, device=device))

        # ç›‘æ§ç»Ÿè®¡
        self.register_buffer("flip_counter", torch.zeros(num_bits, dtype=torch.int32, device=device))
        self.register_buffer("class_counts", torch.zeros(num_classes, dtype=torch.int32, device=device))
        self.register_buffer("branch_ema_occ", torch.zeros(num_branches, dtype=torch.float32, device=device))

        # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šä¿ç•™åŸºçº¿å ç”¨ç‡ç”¨äºç›‘æµ‹ï¼ˆä¸åšæ§åˆ¶ï¼‰
        self.register_buffer("baseline_occ_per_class", torch.zeros(num_classes, dtype=torch.float32, device=device))

        # EMA + ç¨³æ€åŒ–å‚æ•°
        self.ema_momentum = max(0.5, min(0.999, 0.9))
        self.eta_homeo = max(1e-3, min(0.2, 0.05))
        self.p_gate_min = 1e-4

        # å…¼å®¹æ€§åˆ«åï¼ˆè¯Šæ–­è„šæœ¬ä½¿ç”¨ï¼‰
        self.C = num_classes
        self.N = num_bits
        self.B = num_branches
        
        # homeostasisç›®æ ‡å ç”¨ç‡
        self.alpha_star = 0.025  # 2.5%ç›®æ ‡å ç”¨ç‡
        
        # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šæ·»åŠ T_eff bufferç”¨äºæ–°é—¨æ§ç­–ç•¥
        T_eff_val = tau_e_steps * math.log(1.0 / theta) if theta > 0 else 10.0
        self.register_buffer("T_eff", torch.tensor(T_eff_val, dtype=torch.float32, device=device))

        # T_eff buffer
        tau_e_val = float(tau_e_steps)
        theta_val = max(float(theta), 1e-6)
        self.register_buffer("T_eff", torch.tensor(tau_e_val * math.log(1.0 / theta_val),
                                                   dtype=torch.float32, device=device))
        # ä½¿ç”¨æ¨å¯¼çš„99%é¥±å’Œä¸Šé™
        self.p_gate_max = min(0.5, math.log(100.0) / self.T_eff.item())

        self._branch_index = None
        
    # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šå·²ç§»é™¤update_derived_thresholdsæ–¹æ³•
    # def update_derived_thresholds(self, p_pre: float): # å·²ç§»é™¤

    # ---------- æ£€ç´¢ ----------
    @torch.no_grad()
    def retrieve(self, x_bits: torch.Tensor, update_z: bool = False) -> torch.Tensor:
        """
        BTSPè®°å¿†æ£€ç´¢ï¼šäºŒè¿›åˆ¶æ¿€æ´»æ¨¡å¼ â†’ ç±»åˆ«logits
        
        æµç¨‹ï¼š
        1. popcountè®¡ç®—åŸå§‹åˆ†æ•°: scores = x_bits @ S^T
        2. z-scoreæ ‡å‡†åŒ–: z = (scores - Î¼) / Ïƒ 
        3. æ¸©åº¦ç¼©æ”¾è¾“å‡º: logits = z / T
        
        Args:
            x_bits: [B, N] torch.bool
                äºŒè¿›åˆ¶æ¿€æ´»æ¨¡å¼ï¼Œå¿…é¡»åœ¨CPUä¸Šä¸”ä¸è®°å¿†ç»´åº¦NåŒ¹é…
            update_z: bool, default=False  
                æ˜¯å¦æ›´æ–°z-scoreç»Ÿè®¡é‡(Î¼, Ïƒ)ï¼Œè®­ç»ƒ/è¯„æµ‹é˜¶æ®µç­–ç•¥ä¸åŒï¼š
                
                ğŸ‹ï¸ è®­ç»ƒé˜¶æ®µï¼šå»ºè®® update_z=True
                - å…è®¸ç»Ÿè®¡é‡åŠ¨æ€é€‚åº”æ•°æ®åˆ†å¸ƒå˜åŒ–
                - å®šæœŸæ›´æ–°(å¦‚æ¯å‡ ä¸ªepoch)ä»¥ä¿æŒå‡†ç¡®æ€§
                - æ”¯æŒå¢é‡å­¦ä¹ ä¸­çš„åˆ†å¸ƒæ¼‚ç§»é€‚åº”
                
                è¯„æµ‹é˜¶æ®µï¼šå¼ºåˆ¶ update_z=False  
                - å†»ç»“ç»Ÿè®¡é‡é¿å…æµ‹è¯•æ•°æ®æ³„æ¼
                - ç¡®ä¿å¯é‡ç°çš„è¯„æµ‹ç»“æœ
                - éµå¾ªæ ‡å‡†MLè¯„æµ‹åè®®
                
        Returns:
            torch.Tensor: [B, C] torch.float32
                ç±»åˆ«logitsï¼Œå·²åº”ç”¨z-scoreæ ‡å‡†åŒ–å’Œæ¸©åº¦ç¼©æ”¾ï¼Œåœ¨CPUä¸Š
                
        Raises:
            AssertionError: å½“x_bitsç±»å‹ã€ç»´åº¦æˆ–å°ºå¯¸ä¸åŒ¹é…æ—¶
            
        Training/Evaluation Protocol:
            æ¨èçš„è°ƒç”¨æ¨¡å¼ï¼š
            
            ```python
            # è®­ç»ƒé˜¶æ®µï¼šå…è®¸ç»Ÿè®¡é‡æ›´æ–°
            model.train()
            for epoch in range(num_epochs):
                for batch_idx, (data, labels) in enumerate(train_loader):
                    # æ¯Nä¸ªbatchæ›´æ–°ä¸€æ¬¡z-statsï¼Œé¿å…è¿‡äºé¢‘ç¹
                    update_stats = (batch_idx % 50 == 0)  
                    logits = btsp.retrieve(x_bits, update_z=update_stats)
                    
            # è¯„æµ‹é˜¶æ®µï¼šå†»ç»“ç»Ÿè®¡é‡
            model.eval()
            with torch.no_grad():
                for data, labels in test_loader:
                    logits = btsp.retrieve(x_bits, update_z=False)  # æ˜¾å¼å†»ç»“
            ```
            
            æˆ–ä½¿ç”¨è®­ç»ƒçŠ¶æ€è‡ªåŠ¨åˆ‡æ¢ï¼š
            ```python
            # è‡ªåŠ¨æ ¹æ®æ¨¡å‹çŠ¶æ€å†³å®šupdate_z
            update_z = self.training  # PyTorchçš„trainingçŠ¶æ€
            logits = btsp.retrieve(x_bits, update_z=update_z)
            ```
            
        Data Leakage Prevention:
            å…³é”®ï¼šè¯„æµ‹æ—¶å¿…é¡»è®¾ç½®update_z=False
            - æµ‹è¯•æ•°æ®ä¸åº”å½±å“æ¨¡å‹å†…éƒ¨ç»Ÿè®¡é‡
            - ç¡®ä¿è®­ç»ƒå’Œæµ‹è¯•çš„ä¸¥æ ¼åˆ†ç¦»
            - é¿å…æ— æ„ä¸­çš„ä¿¡æ¯æ³„æ¼å¯¼è‡´è™šé«˜æ€§èƒ½
            
        Performance Impact:
            - update_z=True: é¢å¤–O(BÃ—C)è®¡ç®—å¼€é”€ï¼ŒEMAæ›´æ–°ç»Ÿè®¡é‡
            - update_z=False: æ— é¢å¤–å¼€é”€ï¼Œçº¯å‰å‘æ¨ç†
            - å»ºè®®è®­ç»ƒæ—¶é€‚åº¦æ›´æ–°ï¼ˆå¦‚æ¯50æ­¥ï¼‰ï¼Œé¿å…è®¡ç®—æµªè´¹
            
        Note:
            - ç©ºbatch (B=0) è¿”å›å½¢çŠ¶æ­£ç¡®çš„é›¶å¼ é‡
            - ä½¿ç”¨è‡ªé€‚åº”z-scoreä¸‹ç•Œ: max(1e-6, 1/âˆšB) æé«˜å°æ‰¹æ¬¡ç¨³å®šæ€§
            - å†…ç½®NaNé˜²æŠ¤ç¡®ä¿æ•°å€¼å®‰å…¨æ€§
            - ç»Ÿè®¡é‡æ›´æ–°ä½¿ç”¨EMAå¹³æ»‘ï¼Œé¿å…å•æ‰¹æ¬¡å™ªå£°å½±å“
        """
        # ç©ºbatchå¿«é€Ÿè¿”å›
        if x_bits.numel() == 0:
            return torch.zeros(0, self.num_classes, dtype=torch.float32, device=x_bits.device)
            
        assert x_bits.dtype == torch.bool and x_bits.dim() == 2 and x_bits.size(1) == self.num_bits
        
        # ç¡®ä¿è®¾å¤‡å…¼å®¹æ€§
        x_bits_device = x_bits.to(self.S.device)
        
        # popcount ç­‰ä»·äºå¸ƒå°”ç‚¹ç§¯
        scores = (x_bits_device.float() @ self.S.transpose(0, 1).float())  # [B,C]
        if update_z:
            self.update_zstats(scores)
        
        # è‡ªé€‚åº”z-scoreä¸‹ç•Œï¼šåŸºç¡€ä¸‹ç•Œ + æ ·æœ¬æ•°è‡ªé€‚åº”é¡¹
        B = x_bits.size(0)
        adaptive_min_std = max(1e-6, 1.0 / (B ** 0.5)) if B > 0 else 1e-6
        z_std_safe = self.z_std.clamp_min(adaptive_min_std)
        
        z = (scores - self.z_mu) / z_std_safe
        # NaNé˜²æŠ¤ï¼šç¡®ä¿æ•°å€¼å®‰å…¨
        z = torch.nan_to_num(z, nan=0.0, posinf=10.0, neginf=-10.0)
        
        return (z / self.temperature)

    @torch.no_grad()
    def raw_scores(self, x_bits: torch.Tensor) -> torch.Tensor:
        """è¿”å›æœªå½’ä¸€åŒ–çš„ popcount å¾—åˆ† (x_bits @ S^T)ã€‚
        ç”¨äº margin / è¯Šæ–­ï¼Œå¯ä¸ retrieve() åŒºåˆ†ï¼ˆåè€…å« z-score ä¸æ¸©åº¦ï¼‰ã€‚
        Args:
            x_bits: [B,N] bool
        Returns:
            scores: [B,C] float32
        """
        if x_bits.numel() == 0:
            return torch.zeros(0, self.num_classes, dtype=torch.float32, device=self.S.device)
        assert x_bits.dtype == torch.bool and x_bits.size(1) == self.num_bits
        
        # ç¡®ä¿è®¾å¤‡å…¼å®¹æ€§
        x_bits_device = x_bits.to(self.S.device)
        return (x_bits_device.float() @ self.S.transpose(0,1).float())

    @torch.no_grad()
    def stats_snapshot(self) -> dict:
        """é‡‡é›†å½“å‰ BTSP è®°å¿†çŠ¶æ€çš„è½»é‡ç»Ÿè®¡ï¼ˆç”¨äºæ—¥å¿—/ç›‘æ§ï¼‰ã€‚"""
        occ_per_branch = None
        if self._branch_index is None:
            self._ensure_branch_index()
        # ä¿®å¤ï¼šnum_brancheså¯èƒ½æ˜¯intè€Œétensor
        Bn = int(self.num_branches.item()) if hasattr(self.num_branches, 'item') else int(self.num_branches)
        occ_vals = []
        for b in range(Bn):
            I = self._branch_index[b]
            if I.numel() > 0:
                occ_vals.append(self.S[:, I].float().mean().item())
            else:
                occ_vals.append(0.0)
        occ_per_branch = occ_vals
        flips_total = int(self.flip_counter.sum().item())
        flips_mean = float(self.flip_counter.float().mean().item())
        return {
            "C": self.num_classes,
            "N": self.num_bits,
            "B": self.num_branches,
            "p_gate_mean": float(self.p_gate.mean().item()),
            "p_gate_std": float(self.p_gate.std().item()),
            "theta": float(self.theta.item()) if hasattr(self.theta, 'item') else float(self.theta),
            "T_eff": float(self.T_eff.item()) if hasattr(self.T_eff, 'item') else float(self.T_eff),
            "alpha_star": float(self.alpha_star),
            "occ_branch": occ_per_branch,
            "occ_mean": float(torch.tensor(occ_per_branch).mean().item()) if len(occ_per_branch)>0 else 0.0,
            "flips_total": flips_total,
            "flips_mean_per_bit": flips_mean,
            "class_counts_sum": int(self.class_counts.sum().item())
        }

    @torch.no_grad()
    def update_zstats(self, scores: torch.Tensor, momentum: float | None = None) -> None:
        """
        æ›´æ–°z-scoreæ ‡å‡†åŒ–ç»Ÿè®¡é‡ (Î¼, Ïƒ)
        
        ä½¿ç”¨EMAå¹³æ»‘æ›´æ–°é¿å…å•æ‰¹æ¬¡å™ªå£°å½±å“ï¼š
        - Î¼_new = momentum * Î¼_old + (1 - momentum) * Î¼_batch
        - Ïƒ_new = momentum * Ïƒ_old + (1 - momentum) * Ïƒ_batch
        
        Args:
            scores: [B, C] torch.float32
                å½“å‰æ‰¹æ¬¡çš„åŸå§‹æ£€ç´¢åˆ†æ•° (popcountç»“æœ)
            momentum: float | None, default=None
                EMAåŠ¨é‡å‚æ•° âˆˆ [0, 1]ï¼ŒNoneæ—¶ä½¿ç”¨self.z_momentum
                è¶Šæ¥è¿‘1è¶Šå¹³æ»‘ï¼Œè¶Šæ¥è¿‘0å¯¹å½“å‰æ‰¹æ¬¡è¶Šæ•æ„Ÿ
                
        Note:
            - ç©ºbatchæ—¶ç›´æ¥è¿”å›ï¼Œä¸æ›´æ–°ç»Ÿè®¡é‡
            - å†…ç½®NaNé˜²æŠ¤ç¡®ä¿ç»Ÿè®¡é‡æ•°å€¼å®‰å…¨
            - å»ºè®®åœ¨è®­ç»ƒæ—¶æ¯å‡ ä¸ªepochè°ƒç”¨ä¸€æ¬¡ä»¥ä¿æŒç»Ÿè®¡é‡æ›´æ–°
        """
        if scores.numel() == 0:
            return
            
        m = self.z_momentum if momentum is None else float(momentum)
        mu = scores.mean(dim=0)
        std = scores.std(dim=0, unbiased=False)
        
        # NaNé˜²æŠ¤ï¼šç¡®ä¿ç»Ÿè®¡é‡æ•°å€¼å®‰å…¨
        mu = torch.nan_to_num(mu, nan=0.0)
        std = torch.nan_to_num(std, nan=1e-6)
        
        self.z_mu.mul_(m).add_(mu * (1 - m))
        self.z_std.mul_(m).add_(std * (1 - m))

    def bytes_per_class(self) -> int:
        return (self.num_bits + 7) // 8

    @torch.no_grad()
    def retrieve_auto(self, x_bits: torch.Tensor, force_update_z: bool | None = None) -> torch.Tensor:
        """
        è‡ªåŠ¨æ¨¡å¼æ£€ç´¢ï¼šæ ¹æ®æ¨¡å—è®­ç»ƒçŠ¶æ€æ™ºèƒ½å†³å®šæ˜¯å¦æ›´æ–°z-scoreç»Ÿè®¡é‡
        
        è®¾è®¡ç†å¿µ:
        - è®­ç»ƒæ¨¡å¼(self.training=True): é»˜è®¤å¯ç”¨ç»Ÿè®¡é‡æ›´æ–°
        - è¯„æµ‹æ¨¡å¼(self.training=False): é»˜è®¤å†»ç»“ç»Ÿè®¡é‡ï¼Œé¿å…æ•°æ®æ³„æ¼
        - å¯é€šè¿‡force_update_zå‚æ•°å¼ºåˆ¶è¦†ç›–è‡ªåŠ¨è¡Œä¸º
        
        Args:
            x_bits: [B, N] torch.bool
                äºŒè¿›åˆ¶æ¿€æ´»æ¨¡å¼ï¼Œå¿…é¡»åœ¨CPUä¸Šä¸”ä¸è®°å¿†ç»´åº¦NåŒ¹é…
            force_update_z: bool | None, default=None
                å¼ºåˆ¶è¦†ç›–è‡ªåŠ¨update_zå†³ç­–ï¼š
                - None: è‡ªåŠ¨æ¨¡å¼ï¼Œæ ¹æ®self.trainingå†³å®š
                - True: å¼ºåˆ¶æ›´æ–°ç»Ÿè®¡é‡ï¼ˆå¿½ç•¥è®­ç»ƒçŠ¶æ€ï¼‰
                - False: å¼ºåˆ¶å†»ç»“ç»Ÿè®¡é‡ï¼ˆå¿½ç•¥è®­ç»ƒçŠ¶æ€ï¼‰
                
        Returns:
            torch.Tensor: [B, C] torch.float32
                ç±»åˆ«logitsï¼Œå·²åº”ç”¨z-scoreæ ‡å‡†åŒ–å’Œæ¸©åº¦ç¼©æ”¾
                
        Usage Pattern:
            ```python
            # è‡ªåŠ¨æ¨¡å¼ï¼šæ¨èç”¨æ³•
            model.train()  # è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
            logits = btsp.retrieve_auto(x_bits)  # è‡ªåŠ¨update_z=True
            
            model.eval()   # è®¾ç½®ä¸ºè¯„æµ‹æ¨¡å¼  
            logits = btsp.retrieve_auto(x_bits)  # è‡ªåŠ¨update_z=False
            
            # å¼ºåˆ¶æ§åˆ¶ï¼šç‰¹æ®Šéœ€æ±‚
            logits = btsp.retrieve_auto(x_bits, force_update_z=False)  # å¼ºåˆ¶å†»ç»“
            ```
            
        Note:
            - å†…éƒ¨è°ƒç”¨æ ‡å‡†retrieve()æ–¹æ³•ï¼Œç»§æ‰¿æ‰€æœ‰å®‰å…¨ç‰¹æ€§
            - è®­ç»ƒçŠ¶æ€é€šè¿‡PyTorchçš„self.trainingå±æ€§è‡ªåŠ¨æ£€æµ‹
            - æ¨èåœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹ä½¿ç”¨æ­¤æ–¹æ³•ï¼Œå‡å°‘æ‰‹åŠ¨å‚æ•°ç®¡ç†
        """
        if force_update_z is not None:
            update_z = bool(force_update_z)
        else:
            # è‡ªåŠ¨æ¨¡å¼ï¼šè®­ç»ƒæ—¶æ›´æ–°ï¼Œè¯„æµ‹æ—¶å†»ç»“
            update_z = self.training
            
        return self.retrieve(x_bits, update_z=update_z)

    # ---------- å†™å…¥ ----------
    @torch.no_grad()
    def write(self, x_bits: torch.Tensor, y: torch.Tensor, tau_e: float | None = None) -> None:
        """
        BTSPè®°å¿†å†™å…¥ï¼šé€šè¿‡é€‰æ‹©æ€§XORç¿»è½¬æ›´æ–°è®°å¿†çŠ¶æ€
        
        æ ¸å¿ƒç®—æ³• - SFS-BTSP (Sparse Flip-Set Binary Temporal Sparse Projection):
        1. èµ„æ ¼è¿¹è¡°å‡: e â† max(Î²Â·e, âˆªx_bits) å…¶ä¸­ Î² = exp(-1/Ï„_e)
        2. åˆ†æ”¯é—¨æ§: g_b ~ Bernoulli(p_eff), p_eff = 1 - exp(-p_gateÂ·T_eff)  
        3. æ¡ä»¶ç¿»è½¬: è‹¥ eligible & fired & coin(0.5)ï¼Œåˆ™ S[c,i] â† S[c,i] âŠ• 1
        4. æŒ‰ç±»æˆªæ–­: ä»…å¯¹å½“å‰æ‰¹æ¬¡ä¸­è¯¥ç±»æ¿€æ´»çš„ä½è¿›è¡Œç¿»è½¬
        
        Args:
            x_bits: [B, N] torch.bool
                äºŒè¿›åˆ¶æ¿€æ´»ä½æ¨¡å¼ï¼Œå¿…é¡»åœ¨CPUä¸Šä¸”ä¸è®°å¿†ç»´åº¦NåŒ¹é…
                æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬çš„Nç»´äºŒè¿›åˆ¶ç‰¹å¾
                è‹¥x_bitsåœ¨GPUä¸Šï¼Œå°†æŠ›å‡ºæ–­è¨€é”™è¯¯
            y: [B] torch.long  
                ç±»åˆ«æ ‡ç­¾ï¼ŒèŒƒå›´åº”åœ¨ [0, C-1]ï¼Œå…¶ä¸­Cä¸ºè®°å¿†çš„ç±»åˆ«æ•°
                è‹¥yåœ¨GPUä¸Šï¼Œå°†æŠ›å‡ºæ–­è¨€é”™è¯¯
            tau_e: float | None, default=None
                èµ„æ ¼è¿¹æ—¶é—´å¸¸æ•°ï¼ˆæ­¥æ•°ï¼‰ï¼Œæ§åˆ¶å†å²ä¿¡æ¯è¡°å‡é€Ÿåº¦
                Noneæ—¶ä½¿ç”¨åˆå§‹åŒ–çš„tau_e_stepså€¼ï¼Œæ¨èèŒƒå›´[1.0, 100.0]
                
        Side Effects:
            - æ›´æ–°è®°å¿†çŸ©é˜µ S[C, N] 
            - æ›´æ–°èµ„æ ¼è¿¹å‘é‡ e[N]
            - å¢åŠ ç¿»è½¬è®¡æ•°å™¨ flip_counter[N]
            - å¢åŠ ç±»åˆ«æ ·æœ¬è®¡æ•° class_counts[C]
            
        Note:
            - ç©ºbatchæ—¶ç›´æ¥è¿”å›ï¼Œä¸æ‰§è¡Œä»»ä½•æ“ä½œ
            - ä½¿ç”¨"æŒ‰ç±»æˆªæ–­"ç­–ç•¥ï¼šä»…ç¿»è½¬è¯¥ç±»åœ¨å½“å‰æ‰¹æ¬¡ä¸­æ¿€æ´»çš„ä½
            - XORç¿»è½¬æ˜¯å¯é€†çš„ï¼Œæ”¯æŒå¢é‡é—å¿˜æœºåˆ¶
            - åˆ†æ”¯é—¨æ§æ¦‚ç‡ç”±Îµ-controlç­–ç•¥åŠ¨æ€è°ƒèŠ‚
        """
        # ç©ºbatchå¿«é€Ÿè¿”å›
        if x_bits.numel() == 0:
            return
            
        assert x_bits.dtype == torch.bool and y.dtype == torch.long
        
        # è®¾å¤‡ä¸€è‡´æ€§æ˜¾å¼æ–­è¨€ï¼šé˜²æ­¢CPU/GPUç´¢å¼•é”™é…
        dev = self.S.device
        assert x_bits.device.type == "cpu" and y.device.type == "cpu", \
            f"BTSPMemory expects CPU tensors; got x_bits:{x_bits.device}, y:{y.device}, S:{dev}"
        
        # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šä»…é€šè¿‡p_gateæ§åˆ¶ï¼Œç§»é™¤æ‰€æœ‰ç¡¬åœ/è½¯å›é€€é€»è¾‘
        # è®¡ç®—å½“å‰å ç”¨ç‡ç”¨äºç›‘æµ‹ï¼ˆä¸åšé˜ˆå€¼æ§åˆ¶ï¼‰
        occ_per_class = self.S.float().mean(dim=1)  # [C] æ¯ç±»å ç”¨ç‡
        global_occ = occ_per_class.mean().item()
        
        # Light assertion: warn when occupancy is high but don't block (reduced frequency)
        if global_occ > 0.12:  # Raised threshold to reduce noise
            import logging
            logging.warning(f"[BTSP] Occupancy monitor: global_occ={global_occ:.4f} > 0.12, controlled by pure gating")
        
        # Warn when gate rate approaches limit
        p_gate_mean = self.p_gate.mean().item()
        p_gate_cap = 0.5  # Simplified limit check
        if p_gate_mean > 0.8 * p_gate_cap:
            import logging
            logging.warning(f"[BTSP] Gate rate monitor: p_gate={p_gate_mean:.6f} approaching limit, controlled by pure gating")
        
        # èµ„æ ¼è¿¹æŒ‰ç±»åˆ«ç»´æŠ¤ï¼šä¿®å¤å…¨å±€å…±äº«é—®é¢˜
        tau = float(self.tau_e_steps.item()) if tau_e is None else float(tau_e)  # é»˜è®¤ä½¿ç”¨bufferå€¼
        beta = math.exp(-1.0 / max(tau, 1.0))
        
        # ç¡®ä¿èµ„æ ¼è¿¹æ˜¯æŒ‰ç±»åˆ«çš„ [C, N] 
        if not hasattr(self, 'e_c') or self.e_c.shape != (self.num_classes, self.num_bits):
            self.e_c = torch.zeros(self.num_classes, self.num_bits, device=self.S.device, dtype=torch.float32)
        
        # å…ˆå¯¹æ‰€æœ‰ç±»æ‰§è¡Œè¡°å‡
        self.e_c.mul_(beta)
        
        # æŒ‰ç±»æ›´æ–°èµ„æ ¼è¿¹ï¼šåªå¯¹è¯¥ç±»åœ¨å½“å‰æ‰¹æ¬¡æ¿€æ´»çš„ä½ç½®ä½
        for i, cls in enumerate(y.tolist()):
            x_i = x_bits[i].to(self.S.device).float()  # [N] float
            self.e_c[cls] = torch.maximum(self.e_c[cls], x_i)

        # åˆ†æ”¯é—¨æ§å‚æ•°
        Bn = int(self.num_branches)
        p_gate_safe = torch.clamp(self.p_gate, 0.0, 10.0)  # é˜²æ­¢æç«¯å€¼
        T_eff_safe = max(self.T_eff.item(), 1e-6)           # é˜²æ­¢0é™¤
        
        exp_term = torch.exp(-p_gate_safe * T_eff_safe)
        exp_term = torch.nan_to_num(exp_term, nan=1.0, posinf=1.0, neginf=0.0)  # NaNé˜²æŠ¤
        p_eff = 1.0 - exp_term   # [B]
        
        gate = (torch.rand(Bn, device=self.S.device) < p_eff) # [B]
        fired = gate[self.branch_of]                          # [N]

        # æŒ‰ç±»ç”Ÿæˆç¿»è½¬æ©ç å¹¶æ‰§è¡Œï¼šä¿®å¤å…¨å±€æ©ç é—®é¢˜
        applied = torch.zeros(self.num_bits, dtype=torch.bool, device=self.S.device)
        for cls in torch.unique(y).tolist():
            # è¯¥ç±»çš„èµ„æ ¼è¿¹ã€éšæœºå¸ã€æŒ‰ç±»æ¿€æ´»ä½
            elig_c = (self.e_c[cls] > self.theta)                                        # [N]
            coin_c = (torch.rand(self.num_bits, device=self.S.device) < 0.5)                   # [N] 
            mask_c = (y == cls)                                                          # [B]
            x_c_any = x_bits[mask_c].any(dim=0).to(self.S.device) if mask_c.any() else torch.zeros(self.num_bits, dtype=torch.bool, device=self.S.device)  # [N]
            
            # è¯¥ç±»çš„ç¿»è½¬æ©ç ï¼šèµ„æ ¼è¿¹ & é—¨æ§ & éšæœºå¸ & è¯¥ç±»æ¿€æ´»ä½
            flip_mask_c = elig_c & fired & coin_c & x_c_any
            
            # åªå¯¹è¯¥ç±»æ‰§è¡Œç¿»è½¬
            self.S[cls] ^= flip_mask_c
            applied |= flip_mask_c
            self.class_counts[cls] += mask_c.sum().item()
            
        # flip_counter è®¡ç®—å®é™…ç¿»è½¬ä½æ•°
        self.flip_counter += applied.int()

    # ---------- ç¨³æ€ï¼ˆåˆ†æ”¯å ç”¨ EMA -> æŒ‡æ•°è°ƒ p_gateï¼‰ ----------
    @torch.no_grad()
    def homeostasis_step(self) -> dict[str, float]:
        """
        BTSPç¨³æ€è°ƒèŠ‚ï¼šç»´æŒè®°å¿†ç³»ç»Ÿçš„åŠ¨æ€å¹³è¡¡
        
        æ ¸å¿ƒæœºåˆ¶ - åˆ†æ”¯å ç”¨ç‡åé¦ˆæ§åˆ¶:
        1. ç»Ÿè®¡æ¯ä¸ªåˆ†æ”¯çš„å®é™…å ç”¨ç‡: occ[b] = mean(S[:, branch_b])
        2. EMAå¹³æ»‘æ›´æ–°: occ_ema[b] â† Î±Â·occ_ema[b] + (1-Î±)Â·occ[b]  
        3. è®¡ç®—åå·®: Î´[b] = Î±_target - occ_ema[b]
        4. æŒ‡æ•°è°ƒæ•´é—¨æ§ç‡: p_gate[b] â† p_gate[b] Â· exp(Î·Â·Î´[b])
        5. çº¦æŸåˆ°å®‰å…¨èŒƒå›´: p_gate[b] â† clamp(p_gate[b], p_min, p_max)
        
        ç›®æ ‡ï¼š
        - ç»´æŒå„åˆ†æ”¯å ç”¨ç‡æ¥è¿‘ç›®æ ‡å€¼ alpha_star (é€šå¸¸~2-5%)
        - é¿å…æŸäº›åˆ†æ”¯è¿‡åº¦æ¿€æ´»å¯¼è‡´è®°å¿†å®¹é‡ä¸å‡
        - é€šè¿‡è´Ÿåé¦ˆå®ç°é•¿æœŸç¨³å®šæ€§
        
        è°ƒç”¨æ—¶æœºå»ºè®®:
        **å®šæœŸè°ƒç”¨ç­–ç•¥** (æ¨è):
        - æ¯100-500ä¸ªè®­ç»ƒbatchè°ƒç”¨ä¸€æ¬¡
        - æ¯ä¸ªepochç»“æŸåè°ƒç”¨ä¸€æ¬¡
        - æ–°ä»»åŠ¡å¼€å§‹æ—¶å‰å‡ ä¸ªepochå¢åŠ é¢‘ç‡
        
        **è‡ªé€‚åº”è°ƒç”¨ç­–ç•¥** (é«˜çº§):
        - ç›‘æ§å ç”¨ç‡æ–¹å·®ï¼Œé«˜æ–¹å·®æ—¶å¢åŠ é¢‘ç‡
        - åŸºäºé—¨æ§ç‡å˜åŒ–å¹…åº¦åŠ¨æ€è°ƒæ•´
        - æŸå¤±plateauæ—¶è§¦å‘ç¨³æ€è°ƒèŠ‚
        
        ğŸš« **é¿å…åœºæ™¯**:
        - æ¨ç†/è¯„æµ‹é˜¶æ®µæ— éœ€è°ƒç”¨
        - è¿‡äºé¢‘ç¹è°ƒç”¨(æ¯ä¸ªbatch)ä¼šå½±å“æ€§èƒ½
        - åˆå§‹åŒ–åç«‹å³è°ƒç”¨(éœ€è¦ä¸€å®šæ•°æ®ç§¯ç´¯)
        
        Returns:
            dict[str, float]: ç¨³æ€ç›‘æ§æŒ‡æ ‡ï¼Œç”¨äºæ—¥å¿—å’Œè°ƒå‚
            {
                'occ_mean': å½“å‰åˆ†æ”¯å ç”¨ç‡å‡å€¼,
                'occ_std': å½“å‰åˆ†æ”¯å ç”¨ç‡æ ‡å‡†å·®,  
                'occ_target': ç›®æ ‡å ç”¨ç‡ alpha_star,
                'p_gate_mean': è°ƒæ•´åé—¨æ§ç‡å‡å€¼,
                'p_gate_std': è°ƒæ•´åé—¨æ§ç‡æ ‡å‡†å·®,
                'p_gate_min': æœ€å°é—¨æ§ç‡,
                'p_gate_max': æœ€å¤§é—¨æ§ç‡,
                'adjustment_strength': æœ¬æ¬¡è°ƒæ•´å¼ºåº¦ (exp_factorçš„logå‡å€¼)
            }
            
        Usage Pattern:
            ```python
            # å®šæœŸè°ƒç”¨æ¨¡å¼
            for epoch in range(num_epochs):
                for batch_idx, (data, labels) in enumerate(train_loader):
                    # ... è®­ç»ƒé€»è¾‘ ...
                    
                    # æ¯100ä¸ªbatchè°ƒç”¨ä¸€æ¬¡
                    if batch_idx % 100 == 0:
                        stats = btsp.homeostasis_step()
                        logger.info(f"Homeostasis: occ={stats['occ_mean']:.4f}Â±{stats['occ_std']:.4f}, "
                                  f"p_gate={stats['p_gate_mean']:.6f}Â±{stats['p_gate_std']:.6f}")
                
                # æ¯ä¸ªepochç»“æŸåä¹Ÿè°ƒç”¨ä¸€æ¬¡
                stats = btsp.homeostasis_step()
                logger.info(f"Epoch {epoch} Homeostasis: {stats}")
            ```
            
        Monitoring Guidelines:
            - occ_mean â‰ˆ alpha_star: ç³»ç»Ÿè¾¾åˆ°å¹³è¡¡
            - occ_std < 0.01: åˆ†æ”¯å ç”¨ç‡å‡åŒ€
            - p_gate_std < 0.1: é—¨æ§ç‡åˆ†å¸ƒåˆç†
            - adjustment_strength â†’ 0: ç³»ç»Ÿè¶‹äºç¨³å®š
            
        Note:
            - å­¦ä¹ ç‡Î·è‡ªåŠ¨çº¦æŸåˆ°[1e-3, 0.2]é˜²æ­¢æŒ¯è¡
            - æŒ‡æ•°å› å­é™åˆ¶åœ¨[exp(-10), exp(10)]é¿å…æ•°å€¼æº¢å‡º
            - åˆ†æ”¯ç´¢å¼•ç¼“å­˜è‡ªåŠ¨ç®¡ç†ï¼Œæ”¯æŒåŠ¨æ€åˆ†æ”¯é‡åˆ†é…
            - å†…ç½®NaNé˜²æŠ¤ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
            - è¿”å›çš„ç›‘æ§æŒ‡æ ‡å»ºè®®è®°å½•åˆ°æ—¥å¿—ä¸­ä»¥ä¾¿åˆ†æ
        """
        self._ensure_branch_index()
        Bn = int(self.num_branches.item()) if hasattr(self.num_branches, 'item') else int(self.num_branches)
        occ = torch.zeros(Bn, dtype=torch.float32, device=self.S.device)
        for b in range(Bn):
            I = self._branch_index[b]
            if I.numel() > 0:
                occ[b] = self.S[:, I].float().mean()  # ç±»Ã—è¯¥åˆ†æ”¯ä½çš„å¹³å‡å ç”¨
        
        # EMAæ›´æ–°å ç”¨ç»Ÿè®¡
        self.branch_ema_occ.mul_(self.ema_momentum).add_(occ * (1 - self.ema_momentum))
        
        # æŒ‡æ•°è°ƒæ•´ï¼Œä½¿ç”¨çº¦æŸçš„å­¦ä¹ ç‡é¿å…æŒ¯è¡ï¼Œæ·»åŠ æ•°å€¼å®‰å…¨é˜²æŠ¤
        delta = self.alpha_star - self.branch_ema_occ
        eta_safe = max(1e-3, min(0.2, self.eta_homeo))  # çº¦æŸå­¦ä¹ ç‡é˜²æ­¢æŒ¯è¡
        
        # é™åˆ¶æŒ‡æ•°å‚æ•°èŒƒå›´ï¼Œé˜²æ­¢æ•°å€¼æº¢å‡º
        exp_arg = torch.clamp(eta_safe * delta, -10.0, 10.0)
        exp_factor = torch.exp(exp_arg)
        # NaNé˜²æŠ¤
        exp_factor = torch.nan_to_num(exp_factor, nan=1.0, posinf=10.0, neginf=0.1)
        
        # è®°å½•è°ƒæ•´å‰çš„é—¨æ§ç‡ç”¨äºè®¡ç®—è°ƒæ•´å¼ºåº¦
        p_gate_before = self.p_gate.clone()
        
        # åº”ç”¨è°ƒæ•´
        self.p_gate.mul_(exp_factor).clamp_(self.p_gate_min, self.p_gate_max)
        
        # åŸºäºå ç”¨é¢„ç®—çš„è½¯å›é€€ï¼ˆæ›¿ä»£å›ºå®š12%é­”æ•°ï¼‰
        occ_per_class = self.S.float().mean(dim=1)  # [C] æ¯ç±»å ç”¨ç‡
        occ_per_branch = torch.zeros(self.num_branches, device=self.S.device)
        for b in range(self.num_branches):
            branch_classes = (self.branch_assignment == b).nonzero(as_tuple=True)[0]
            if len(branch_classes) > 0:
                occ_per_branch[b] = occ_per_class[branch_classes].mean()
        
        # ğŸ”¥ çº¯é—¨æ§ç‰ˆæœ¬ï¼šç§»é™¤å ç”¨é¢„ç®—çº¦æŸé€»è¾‘
        # p_pre = getattr(self, 'p_pre', 0.03125)    # å·²ç§»é™¤
        # alpha_budget = self.c_occ * (p_pre / 2.0)  # å·²ç§»é™¤
        # p_gate_occ_max = ...                       # å·²ç§»é™¤
        # å ç”¨ä¸Šé™çº¦æŸé€»è¾‘å·²ç§»é™¤ï¼Œå®Œå…¨ä¾èµ–ç»Ÿä¸€é—¨æ§è°ƒåº¦å™¨
        
        # è®¡ç®—ç›‘æ§æŒ‡æ ‡
        occ_current = self.branch_ema_occ  # ä½¿ç”¨EMAå¹³æ»‘åçš„å ç”¨ç‡
        adjustment_strength = torch.log(exp_factor).abs().mean().item()
        
        return {
            'occ_mean': float(occ_current.mean().item()),
            'occ_std': float(occ_current.std().item()),
            'occ_target': float(self.alpha_star),
            'p_gate_mean': float(self.p_gate.mean().item()),
            'p_gate_std': float(self.p_gate.std().item()),
            'p_gate_min': float(self.p_gate.min().item()),
            'p_gate_max': float(self.p_gate.max().item()),
            'adjustment_strength': adjustment_strength
        }

    def _ensure_branch_index(self):
        """ç¡®ä¿åˆ†æ”¯ç´¢å¼•ç¼“å­˜æœ‰æ•ˆ"""
        if self._branch_index is None:
            Bn = int(self.num_branches.item()) if hasattr(self.num_branches, 'item') else int(self.num_branches)
            self._branch_index = [(self.branch_of == b).nonzero(as_tuple=True)[0] for b in range(Bn)]

    @torch.no_grad()
    def reassign_branches(self, num_branches: int) -> None:
        """
        é‡æ–°åˆ†é…åˆ†æ”¯ç»“æ„ï¼ˆå®éªŒæ€§åŠŸèƒ½ï¼‰
        
        Args:
            num_branches: æ–°çš„åˆ†æ”¯æ•°é‡
            
        æ³¨æ„ï¼š
        - ä¼šé‡ç½®åˆ†æ”¯EMAç»Ÿè®¡
        - è§¦å‘åˆ†æ”¯ç´¢å¼•ç¼“å­˜é‡å»º
        - å»ºè®®åœ¨ä»»åŠ¡é—´éš”æœŸè°ƒç”¨ï¼Œé¿å…è®­ç»ƒä¸­æ–­
        """
        num_branches = max(1, int(num_branches))
        
        # æ›´æ–°åˆ†æ”¯æ•°é‡å¹¶é‡æ–°éšæœºåˆ†é…
        self.num_branches.fill_(num_branches)
        self.branch_of = torch.randint(0, num_branches, (self.num_bits,), device=self.S.device)
        
        # é‡ç½®ç›¸å…³çŠ¶æ€
        self.branch_ema_occ = torch.zeros(num_branches, dtype=torch.float32, device=self.S.device)
        self.p_gate = torch.full((num_branches,), 0.01, dtype=torch.float32, device=self.S.device)
        
        # è§¦å‘åˆ†æ”¯ç´¢å¼•ç¼“å­˜é‡å»º
        self._branch_index = None
        
        logging.info(f"Branch reassignment complete: {num_branches} branches, state reset")

    @torch.no_grad()
    def set_homeostasis_params(self, 
                             alpha_star: float | None = None,
                             eta_homeo: float | None = None,
                             ema_momentum: float | None = None) -> None:
        """
        è®¾ç½®ç¨³æ€è°ƒèŠ‚å‚æ•°ï¼Œè‡ªåŠ¨åº”ç”¨å®‰å…¨èŒƒå›´çº¦æŸ
        
        Args:
            alpha_star: ç›®æ ‡å ç”¨ç‡ï¼Œçº¦æŸåˆ° [1/N, 0.2] é¿å…æç«¯å€¼
            eta_homeo: å­¦ä¹ ç‡ï¼Œçº¦æŸåˆ° [1e-3, 0.2] é˜²æ­¢æŒ¯è¡  
            ema_momentum: EMAåŠ¨é‡ï¼Œçº¦æŸåˆ° [0.5, 0.999]
        """
        if alpha_star is not None:
            # çº¦æŸç›®æ ‡å ç”¨ç‡ï¼šä¸èƒ½å¤ªä½ï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰ä¹Ÿä¸èƒ½å¤ªé«˜ï¼ˆæ•ˆç‡ä½ï¼‰
            min_alpha = 1.0 / self.num_bits  # æœ€ä½ï¼šæ¯ä½è‡³å°‘æœ‰1/Nçš„æœŸæœ›å ç”¨
            max_alpha = 0.2           # æœ€é«˜ï¼š20%å ç”¨ç‡ï¼Œä¿æŒç¨€ç–æ€§
            self.alpha_star = max(min_alpha, min(max_alpha, float(alpha_star)))
            logging.info(f"Target occupancy set to: {self.alpha_star:.6f} (original: {alpha_star:.6f})")
        
        if eta_homeo is not None:
            # Constrain learning rate: prevent oscillation and numerical instability
            self.eta_homeo = max(1e-3, min(0.2, float(eta_homeo)))
            logging.info(f"Homeostasis learning rate set to: {self.eta_homeo:.6f} (original: {eta_homeo:.6f})")
        
        if ema_momentum is not None:
            # Constrain EMA momentum: maintain reasonable historical smoothing
            self.ema_momentum = max(0.5, min(0.999, float(ema_momentum)))
            logging.info(f"EMA momentum set to: {self.ema_momentum:.6f} (original: {ema_momentum:.6f})")

    # å®ç”¨ï¼šå…¨å±€è®¾é—¨æ§
    @torch.no_grad()
    def set_all_p_gate(self, p: float) -> None:
        self.p_gate.fill_(float(p))
    
    # å®ç”¨ï¼šdtypeéªŒè¯å’Œå»ºè®®
    def validate_dtypes(self, verbose: bool = True) -> dict[str, bool]:
        """
        éªŒè¯BTSPè®°å¿†çš„æ•°æ®ç±»å‹æ˜¯å¦ç¬¦åˆå»ºè®®
        
        Returns:
            dict: å„é¡¹æ£€æŸ¥ç»“æœ
        """
        results = {}
        
        # æ£€æŸ¥è®°å¿†çŸ©é˜µ - åº”è¯¥æ˜¯bool
        results['S_is_bool'] = (self.S.dtype == torch.bool)
        
        # æ£€æŸ¥æµ®ç‚¹å‚æ•° - åº”è¯¥æ˜¯float32
        float32_params = ['e', 'z_mu', 'z_std', 'T_eff', 'p_gate', 'branch_ema_occ']
        for param_name in float32_params:
            if hasattr(self, param_name):
                param = getattr(self, param_name)
                results[f'{param_name}_is_float32'] = (param.dtype == torch.float32)
        
        # æ£€æŸ¥æ•´æ•°ç»Ÿè®¡ - åº”è¯¥æ˜¯int32  
        int32_params = ['flip_counter', 'class_counts', 'num_branches', 'tau_e_steps']
        for param_name in int32_params:
            if hasattr(self, param_name):
                param = getattr(self, param_name)
                results[f'{param_name}_is_int32'] = (param.dtype == torch.int32)
        
        # æ£€æŸ¥æ ‡é‡å‚æ•° - åº”è¯¥æ˜¯float32
        scalar_params = ['theta']
        for param_name in scalar_params:
            if hasattr(self, param_name):
                param = getattr(self, param_name)
                results[f'{param_name}_is_float32'] = (param.dtype == torch.float32)
        
        if verbose:
            print("BTSP Data Type Validation:")
            all_good = True
            for check, passed in results.items():
                status = "OK" if passed else "FAIL"
                print(f"  {check}: {status}")
                if not passed:
                    all_good = False
            
            if all_good:
                print("  All data types conform to recommendations!")
            else:
                print("  Consider correcting dtypes for optimal numerical stability")
                
        return results
    
    # T_eff é‡ç®—æ–¹æ³•ï¼šåœ¨ä¿®æ”¹ theta æˆ– tau_e_steps åè°ƒç”¨
    @torch.no_grad()
    def recompute_teff(self) -> None:
        """
        é‡æ–°è®¡ç®—æœ‰æ•ˆæ¸©åº¦T_effï¼šç»´æŠ¤å‚æ•°ä¸€è‡´æ€§çš„æ ¸å¿ƒæ“ä½œ
        
        åº”ç”¨åœºæ™¯:
        1. ä¿®æ”¹å…³é”®å‚æ•°åçš„å¼ºåˆ¶åŒæ­¥ï¼šthetaæˆ–tau_e_stepså˜æ›´æ—¶å¿…é¡»è°ƒç”¨
        2. æ¢å¤è®­ç»ƒä¼šè¯æ—¶çš„çŠ¶æ€é‡å»ºï¼šç¡®ä¿é—¨æ§æ¦‚ç‡è®¡ç®—æ­£ç¡®æ€§
        3. è¶…å‚æ•°è°ƒä¼˜è¿‡ç¨‹ä¸­çš„å®æ—¶æ›´æ–°ï¼šä¿æŒÎµ-controlç­–ç•¥ä¸€è‡´
        
        è®¡ç®—é€»è¾‘:
        T_eff = Ï„ Â· ln(1/Î¸)ï¼Œå…¶ä¸­ï¼š
        - Ï„: tau_e_stepsï¼Œæ§åˆ¶è®°å¿†è¡°å‡æ—¶é—´å°ºåº¦
        - Î¸: thetaï¼Œæ§åˆ¶æ£€ç´¢é˜ˆå€¼çš„æ•æ„Ÿåº¦
        - å†…å»ºæ•°å€¼ä¿æŠ¤ï¼šmax(Î¸, 1e-6)é˜²æ­¢å¯¹æ•°å‘æ•£
        
        å‰¯ä½œç”¨æ›´æ–°:
        1. ä¿®æ”¹self.T_effç¼“å­˜ï¼Œå½±å“åç»­æ£€ç´¢è¡Œä¸º
        2. æ›´æ–°p_gate_maxä¸Šé™ï¼šmin(0.5, ln(100)/T_eff)ç¡®ä¿99%é¥±å’Œ
        3. è§¦å‘gate_policyæ¨¡å—çš„å‚æ•°é‡æ–°è®¡ç®—
        
        Args:
            æ— å‚æ•°ï¼ŒåŸºäºå½“å‰å¯¹è±¡çŠ¶æ€è®¡ç®—
            
        Returns:
            Noneï¼Œé€šè¿‡side-effectä¿®æ”¹å¯¹è±¡çŠ¶æ€
            
        Performance:
            O(1)å¸¸æ•°æ—¶é—´æ“ä½œï¼Œé€‚ç”¨äºé¢‘ç¹è°ƒç”¨
            
        Critical Usage:
            å‚æ•°ä¿®æ”¹åå¿…é¡»è°ƒç”¨ï¼Œå¦åˆ™å¯¼è‡´é—¨æ§æ¦‚ç‡é”™è¯¯ï¼š
            
            >>> apply_gate_schedule(btsp, eps0, M, p_pre, T_eff)
            >>> btsp.theta.fill_(new_theta)      # ä¿®æ”¹å‚æ•°  
            >>> btsp.recompute_teff()            # â† å¿…é¡»è°ƒç”¨ï¼
            
        Example:
            >>> btsp = BTSPMemory(num_classes=10, num_bits=1024)
            >>> print(f"Initial T_eff: {btsp.T_eff:.4f}")
            >>> btsp.theta.fill_(0.1)            # ä¿®æ”¹é˜ˆå€¼å‚æ•°
            >>> btsp.recompute_teff()            # é‡æ–°è®¡ç®—
            >>> print(f"Updated T_eff: {btsp.T_eff:.4f}")
        """
        tau = float(self.tau_e_steps.item())
        th = float(self.theta.item())
        self.T_eff.fill_(tau * math.log(1.0 / max(th, 1e-6)))
        self.p_gate_max = min(0.5, math.log(100.0) / self.T_eff.item())  # æ›´æ–°99%é¥±å’Œä¸Šé™

    # ç±»æ•°æ‰©å±•ï¼šå¢é‡ä»»åŠ¡æ—¶çš„è¡Œæ‰©å®¹
    @torch.no_grad()
    def expand_classes(self, new_C: int) -> None:
        """
        åŠ¨æ€æ‰©å±•ç±»åˆ«æ•°ï¼šæ”¯æŒå¢é‡å­¦ä¹ ä¸­çš„æ–°ç±»æ·»åŠ 
        
        æ ¸å¿ƒæœºåˆ¶:
        1. æ‰©å®¹è®°å¿†çŸ©é˜µ: S[C, N] â†’ S[new_C, N]ï¼Œæ–°è¡Œåˆå§‹åŒ–ä¸ºFalse
        2. æ‰©å®¹ç»Ÿè®¡å‘é‡: z_mu[C] â†’ z_mu[new_C]ï¼Œæ–°å…ƒç´ åˆå§‹åŒ–ä¸º0  
        3. æ‰©å®¹æ ‡å‡†å·®: z_std[C] â†’ z_std[new_C]ï¼Œæ–°å…ƒç´ åˆå§‹åŒ–ä¸º1
        4. æ‰©å®¹è®¡æ•°å™¨: class_counts[C] â†’ class_counts[new_C]ï¼Œæ–°å…ƒç´ ä¸º0
        5. åŒæ­¥T_effç¼“å­˜ä»¥ä¿æŒÎµ-controlä¸€è‡´æ€§
        
        Args:
            new_C: int
                æ‰©å±•åçš„æ€»ç±»åˆ«æ•°ï¼Œå¿…é¡» â‰¥ å½“å‰ç±»åˆ«æ•°
                æ¨èæŒ‰ä»»åŠ¡é€’å¢ï¼šå¦‚10ç±»/ä»»åŠ¡æ—¶è®¾ä¸º10, 20, 30...
                
        Raises:
            æ— å¼‚å¸¸æŠ›å‡ºï¼Œå½“new_C â‰¤ å½“å‰ç±»æ•°æ—¶é™é»˜è¿”å›
            
        Note:
            - æ–°å¢ç±»åˆ«çš„è®°å¿†åˆå§‹åŒ–ä¸ºå…¨0ï¼Œç­‰å¾…é¦–æ¬¡å­¦ä¹ 
            - z-scoreç»Ÿè®¡é‡ä¿å®ˆåˆå§‹åŒ–(Î¼=0, Ïƒ=1)é¿å…æ£€ç´¢åå‘
            - æ“ä½œåè‡ªåŠ¨è°ƒç”¨recompute_teff()ç¡®ä¿å‚æ•°ä¸€è‡´æ€§
            - æ‰©å±•åçš„è®°å¿†ä¿æŒåœ¨ç›¸åŒè®¾å¤‡ä¸Š
            
        Example:
            >>> btsp = BTSPMemory(num_classes=10, num_bits=1024)
            >>> btsp.expand_classes(20)  # æ·»åŠ 10ä¸ªæ–°ç±»
            >>> print(btsp.num_classes)  # è¾“å‡º: 20
        """
        if new_C <= self.num_classes: 
            return
            
        add = new_C - self.num_classes
        device = self.S.device
        
        # æ‰©å®¹å„ä¸ªçŠ¶æ€çŸ©é˜µ
        pad_S = torch.zeros(add, self.num_bits, dtype=torch.bool, device=device)
        pad_mu = torch.zeros(add, dtype=torch.float32, device=device)
        pad_sd = torch.ones(add, dtype=torch.float32, device=device)
        pad_cc = torch.zeros(add, dtype=torch.int32, device=device)
        pad_baseline = torch.zeros(add, dtype=torch.float32, device=device)

        self.S = torch.cat([self.S, pad_S], dim=0)
        self.z_mu = torch.cat([self.z_mu, pad_mu], dim=0)
        self.z_std = torch.cat([self.z_std, pad_sd], dim=0)
        self.class_counts = torch.cat([self.class_counts, pad_cc], dim=0)
        self.baseline_occ_per_class = torch.cat([self.baseline_occ_per_class, pad_baseline], dim=0)
        
        # æ›´æ–°åˆ†æ”¯ç±»æ˜ å°„
        new_mapping = torch.arange(self.num_classes, new_C, device=device) % self.num_branches
        self.branch_assignment = torch.cat([self.branch_assignment, new_mapping], dim=0)
        
        self.num_classes = new_C
        
    @torch.no_grad()
    def record_task_baseline(self, class_range: tuple[int, int] = None):
        """
        è®°å½•ä»»åŠ¡å¼€å§‹æ—¶çš„åŸºçº¿å ç”¨ç‡ï¼Œç”¨äºå¢é‡é¢„ç®—æ§åˆ¶
        
        Args:
            class_range: (start, end) å½“å‰ä»»åŠ¡çš„ç±»åˆ«èŒƒå›´ï¼Œå¦‚Noneåˆ™è®°å½•æ‰€æœ‰ç±»åˆ«
        """
        current_occ = self.S.float().mean(dim=1)  # [C] å½“å‰æ¯ç±»å ç”¨ç‡
        
        if class_range is None:
            # è®°å½•æ‰€æœ‰ç±»åˆ«
            self.baseline_occ_per_class.copy_(current_occ)
        else:
            start, end = class_range
            end = min(end, self.num_classes)  # é˜²æ­¢è¶Šç•Œ
            if start < end:
                self.baseline_occ_per_class[start:end] = current_occ[start:end]
        
        import logging
        if class_range:
            avg_baseline = self.baseline_occ_per_class[start:end].mean().item()
            logging.info(f"[BTSP] Recorded baseline occupancy for classes {start}-{end}: avg={avg_baseline:.4f}")
        else:
            avg_baseline = self.baseline_occ_per_class.mean().item()
            logging.info(f"[BTSP] Recorded global baseline occupancy: avg={avg_baseline:.4f}")
